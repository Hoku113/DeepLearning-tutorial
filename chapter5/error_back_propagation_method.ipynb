{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5, Error back propagation method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Write calcuration graph\n",
    "\n",
    "\n",
    "#### Practice calcuration graph\n",
    "\n",
    "1. 太郎君はスーパーで1個100円のリンゴを2個買いました。\\\n",
    "支払う金額を求めなさい。ただし、消費税10％を込みで計算すること\n",
    "\n",
    "A. ![image1](./calcuration_graph_image/IMG_2202.png)\n",
    "\n",
    "\n",
    "1. 太郎君はスーパーでリンゴを2個ミカンを3個買いました。 \\\n",
    "リンゴは1個100円、ミカンは1個150円です。消費税10％かかるものとして、支払う金額を求めなさい\n",
    "\n",
    "A.![image2](./calcuration_graph_image/IMG_2203.png)\n",
    "\n",
    "This is a forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Chain rule\n",
    "\n",
    "#### 5-2-1 Composite function\n",
    "$$\n",
    "z = t^2 \\\\\n",
    "t = x + y\n",
    "$$\n",
    "\n",
    "If That function was shown comosite function, it is represented by the product of the derivatives of each of the functions that make up the composite function\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma x} = \\frac{\\sigma z}{\\sigma t}  \\frac{\\sigma t}{\\sigma x} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\frac{\\sigma z}{\\sigma x}\n",
    "$$\n",
    "\n",
    "Calculate $\\frac{\\sigma z}{\\sigma x}$. However You must calculate partitial differential before\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma t} = 2t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma t}{\\sigma x} = 1\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma x} = \\frac{\\sigma z}{\\sigma t} \\frac{\\sigma t}{\\sigma x} = 2t * 1 = 2(x + y)\n",
    "$$\n",
    "\n",
    "Calculate graph\n",
    "![image3](./calcuration_graph_image/IMG_2204.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2-2, How to calculate of  any node\n",
    "\n",
    "##### 1. Addition node\n",
    "- return same inputed value to next node\n",
    "\n",
    "##### 2. Multiplication node\n",
    "- Returns the value that is the flipped value of the input value at the time of forward propagation\n",
    "\n",
    "if you execute this one, you must save the input value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 Make a layer\n",
    "\n",
    "#### 5-3-1, Multilayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward(): Forward propagation\n",
    "# backward(): Backward propagation\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return int(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # flipped x , y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# debug\n",
    "print(int(price))\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-3-2 Addition layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return int(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "110 2.2 3.3 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(int(dapple_num), dapple, round(dorange, 1), dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6, Activation function layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1,ReLU layer\n",
    "\n",
    "- forward ver\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "x & {(x > 0)} \\\\\n",
    "0 & {(x \\geqq 0)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- backward ver\n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = \\begin{cases}\n",
    "1 & (x > 0) \\\\\n",
    "0 & (x \\geqq 0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "# mask: A list of Numpy as True or False\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5],[-2.0, 3.0]])\n",
    "# print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2 sigmoid layer\n",
    "\n",
    "- forward ver\n",
    "$$\n",
    "\\frac{1}{1 + exp(-x)}\n",
    "$$\n",
    "\n",
    "- backward ver\n",
    "\n",
    "### step1\n",
    "\n",
    "\"/\" node is shwon $ y = \\frac{1}{x}$ .\n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = -\\frac{1}{x^2} \\\n",
    " = -y^2\n",
    "$$\n",
    "\n",
    "### step2\n",
    "This is a \"+\" node. Therefore, returns the value obtained in step1 as it is\n",
    "\n",
    "### step3\n",
    "\n",
    "\"exp\" node is shown $ y = exp(x)$. So, in this gradient is \n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = exp(x)\n",
    "$$\n",
    "\n",
    "### step4\n",
    "\"x\" node is invert and multiply the value at that time of forward propagation. \\\n",
    "Therefore, The result is following value\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma L}{\\sigma y} y^2 exp(-x)\n",
    "$$\n",
    "\n",
    "Thus, the formula obtained above can be organaized as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sigma L}{\\sigma y} y^2 exp(-x) &= \\frac{\\sigma L}{\\sigma y}\\frac{1}{(1 + exp(-x))^2}exp(-x) \\\\\n",
    "&= \\frac{\\sigma L}{\\sigma y}\\frac{1}{(1 + exp(-x))}\\frac{exp(-x)}{1 + exp(-x)} \\\\\n",
    "&= \\frac{\\sigma L}{\\sigma y} y(1-y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3 Affine layer\n",
    "\n",
    "In this sentence, Backpropagation of the matrix will be performed\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma L}{\\sigma X} = \\frac{\\sigma L}{\\sigma Y} * W^T \\\\\n",
    "\\frac{\\sigma L}{\\sigma W} = X^T * \\frac{\\sigma L}{\\sigma Y}\n",
    "$$\n",
    "\n",
    "calculation graph\n",
    "![image4](./calcuration_graph_image/IMG_2207.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3-1 Affine layer for batch version\n",
    "\n",
    "Basically the same but, be careful when adding bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "# forward example\n",
    "x_dot_w = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(x_dot_w)\n",
    "\n",
    "print(x_dot_w + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# backward example\n",
    "dy = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "print(dy)\n",
    "\n",
    "db = np.sum(dy, axis=0)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-4 Softmax-with-Loss layer\n",
    "\n",
    "It is usually using output layer\n",
    "\n",
    "example: Mnist data\n",
    "![image5](./calcuration_graph_image/IMG_2209.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Execute back propagation error method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from shared_code.layer import *\n",
    "from shared_code.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                weight_init_std=0.01):\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # Make a layer\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x: input data  t:training data\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # setting\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1 Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from chapter3.dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:5.023503254698496e-10\n",
      "b1:3.1903308851557835e-09\n",
      "W2:4.4588458913617745e-09\n",
      "b2:1.3991031557608613e-07\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Learning using error_back_propagation_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816333333333334 0.9717\n",
      "0.9822 0.972\n",
      "0.98255 0.9731\n",
      "0.98355 0.9712\n",
      "0.9843166666666666 0.9728\n",
      "0.9849666666666667 0.9724\n",
      "0.98545 0.9723\n",
      "0.9867666666666667 0.9729\n",
      "0.9857833333333333 0.9731\n",
      "0.98695 0.9736\n",
      "0.988 0.9735\n",
      "0.9865 0.9727\n",
      "0.98875 0.9737\n",
      "0.9892333333333333 0.9733\n",
      "0.98965 0.9745\n",
      "0.9898 0.9745\n",
      "0.9901666666666666 0.9724\n",
      "0.9917333333333334 0.974\n",
      "0.99045 0.9728\n",
      "0.9902666666666666 0.9732\n",
      "0.9911166666666666 0.9731\n",
      "0.99205 0.9744\n",
      "0.9925666666666667 0.9738\n",
      "0.99345 0.9745\n",
      "0.9933 0.974\n",
      "0.9929333333333333 0.9733\n",
      "0.99315 0.9745\n",
      "0.9933666666666666 0.9744\n",
      "0.9945 0.9748\n",
      "0.9936666666666667 0.9732\n",
      "0.9952833333333333 0.9749\n",
      "0.9946 0.9738\n",
      "0.9955166666666667 0.9744\n",
      "0.9954666666666667 0.9738\n"
     ]
    }
   ],
   "source": [
    "iters_num = 20000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # Calculate the slope using error_back_propagation_method\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # update\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4b6cd190a39ea0ddbc5115907d95646639527cb5a2b4eb6fb2d78053b52a941"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
