{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5, Error back propagation method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Write calcuration graph\n",
    "\n",
    "\n",
    "#### Practice calcuration graph\n",
    "\n",
    "1. 太郎君はスーパーで1個100円のリンゴを2個買いました。\\\n",
    "支払う金額を求めなさい。ただし、消費税10％を込みで計算すること\n",
    "\n",
    "A. ![image1](./calcuration_graph_image/IMG_2202.png)\n",
    "\n",
    "\n",
    "1. 太郎君はスーパーでリンゴを2個ミカンを3個買いました。 \\\n",
    "リンゴは1個100円、ミカンは1個150円です。消費税10％かかるものとして、支払う金額を求めなさい\n",
    "\n",
    "A.![image2](./calcuration_graph_image/IMG_2203.png)\n",
    "\n",
    "This is a forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Chain rule\n",
    "\n",
    "#### 5-2-1 Composite function\n",
    "$$\n",
    "z = t^2 \\\\\n",
    "t = x + y\n",
    "$$\n",
    "\n",
    "If That function was shown comosite function, it is represented by the product of the derivatives of each of the functions that make up the composite function\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma x} = \\frac{\\sigma z}{\\sigma t}  \\frac{\\sigma t}{\\sigma x} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\frac{\\sigma z}{\\sigma x}\n",
    "$$\n",
    "\n",
    "Calculate $\\frac{\\sigma z}{\\sigma x}$. However You must calculate partitial differential before\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma t} = 2t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma t}{\\sigma x} = 1\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma x} = \\frac{\\sigma z}{\\sigma t} \\frac{\\sigma t}{\\sigma x} = 2t * 1 = 2(x + y)\n",
    "$$\n",
    "\n",
    "Calculate graph\n",
    "![image3](./calcuration_graph_image/IMG_2204.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2-2, How to calculate of  any node\n",
    "\n",
    "##### 1. Addition node\n",
    "- return same inputed value to next node\n",
    "\n",
    "##### 2. Multiplication node\n",
    "- Returns the value that is the flipped value of the input value at the time of forward propagation\n",
    "\n",
    "if you execute this one, you must save the input value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 Make a layer\n",
    "\n",
    "#### 5-3-1, Multilayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward(): Forward propagation\n",
    "# backward(): Backward propagation\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return int(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # flipped x , y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# debug\n",
    "print(int(price))\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-3-2 Addition layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return int(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "110 2.2 3.3 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(int(dapple_num), dapple, round(dorange, 1), dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6, Activation function layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1,ReLU layer\n",
    "\n",
    "- forward ver\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "x & {(x > 0)} \\\\\n",
    "0 & {(x \\geqq 0)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- backward ver\n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = \\begin{cases}\n",
    "1 & (x > 0) \\\\\n",
    "0 & (x \\geqq 0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "# mask: A list of Numpy as True or False\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5],[-2.0, 3.0]])\n",
    "# print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2 sigmoid layer\n",
    "\n",
    "- forward ver\n",
    "$$\n",
    "\\frac{1}{1 + exp(-x)}\n",
    "$$\n",
    "\n",
    "- backward ver\n",
    "\n",
    "### step1\n",
    "\n",
    "\"/\" node is shwon $ y = \\frac{1}{x}$ .\n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = -\\frac{1}{x^2} \\\n",
    " = -y^2\n",
    "$$\n",
    "\n",
    "### step2\n",
    "This is a \"+\" node. Therefore, returns the value obtained in step1 as it is\n",
    "\n",
    "### step3\n",
    "\n",
    "\"exp\" node is shown $ y = exp(x)$. So, in this gradient is \n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = exp(x)\n",
    "$$\n",
    "\n",
    "### step4\n",
    "\"x\" node is invert and multiply the value at that time of forward propagation. \\\n",
    "Therefore, The result is following value\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma L}{\\sigma y} y^2 exp(-x)\n",
    "$$\n",
    "\n",
    "Thus, the formula obtained above can be organaized as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sigma L}{\\sigma y} y^2 exp(-x) &= \\frac{\\sigma L}{\\sigma y}\\frac{1}{(1 + exp(-x))^2}exp(-x) \\\\\n",
    "&= \\frac{\\sigma L}{\\sigma y}\\frac{1}{(1 + exp(-x))}\\frac{exp(-x)}{1 + exp(-x)} \\\\\n",
    "&= \\frac{\\sigma L}{\\sigma y} y(1-y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3 Affine layer\n",
    "\n",
    "In this sentence, Backpropagation of the matrix will be performed\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma L}{\\sigma X} = \\frac{\\sigma L}{\\sigma Y} * W^T \\\\\n",
    "\\frac{\\sigma L}{\\sigma W} = X^T * \\frac{\\sigma L}{\\sigma Y}\n",
    "$$\n",
    "\n",
    "calculation graph\n",
    "![image4](./calcuration_graph_image/IMG_2207.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3-1 Affine layer for batch version\n",
    "\n",
    "Basically the same but, be careful when adding bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "# forward example\n",
    "x_dot_w = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(x_dot_w)\n",
    "\n",
    "print(x_dot_w + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# backward example\n",
    "dy = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "print(dy)\n",
    "\n",
    "db = np.sum(dy, axis=0)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-4 Softmax-with-Loss layer\n",
    "\n",
    "It is usually using output layer\n",
    "\n",
    "example: Mnist data\n",
    "![image5](./calcuration_graph_image/IMG_2209.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Execute back propagation error method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from shared_code.layer import *\n",
    "from shared_code.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                weight_init_std=0.01):\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # Make a layer\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x: input data  t:training data\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # setting\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-1 Gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_mnist' from 'shared_code.dataset' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hokut\\デスクトップ\\practice\\DeepLearning-tutorial\\chapter5\\error_back_propagation_method.ipynb Cell 24'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000024?line=1'>2</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpardir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000024?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000024?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mshared_code\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m load_mnist\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_mnist' from 'shared_code.dataset' (unknown location)"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from shared_code.dataset import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hokut\\デスクトップ\\practice\\DeepLearning-tutorial\\chapter5\\error_back_propagation_method.ipynb Cell 25'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39m# load data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000025?line=1'>2</a>\u001b[0m (x_train, t_train), (x_test, y_test) \u001b[39m=\u001b[39m \\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000025?line=2'>3</a>\u001b[0m     load_mnist(normarize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, one_hot_label\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000025?line=4'>5</a>\u001b[0m network \u001b[39m=\u001b[39m TwoLayerNet(input_size\u001b[39m=\u001b[39m\u001b[39m784\u001b[39m, hidden_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hokut/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/practice/DeepLearning-tutorial/chapter5/error_back_propagation_method.ipynb#ch0000025?line=6'>7</a>\u001b[0m x_batch \u001b[39m=\u001b[39m x_train[:\u001b[39m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_mnist' is not defined"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(x_train, t_train), (x_test, y_test) = \\\n",
    "    load_mnist(normarize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=20, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key])) - grad_numerical[key]\n",
    "\n",
    "\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4b6cd190a39ea0ddbc5115907d95646639527cb5a2b4eb6fb2d78053b52a941"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
