{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5, Error back propagation method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Write calcuration graph\n",
    "\n",
    "\n",
    "#### Practice calcuration graph\n",
    "\n",
    "1. 太郎君はスーパーで1個100円のリンゴを2個買いました。\\\n",
    "支払う金額を求めなさい。ただし、消費税10％を込みで計算すること\n",
    "\n",
    "A. ![image1](./calcuration_graph_image/IMG_2202.png)\n",
    "\n",
    "\n",
    "1. 太郎君はスーパーでリンゴを2個ミカンを3個買いました。 \\\n",
    "リンゴは1個100円、ミカンは1個150円です。消費税10％かかるものとして、支払う金額を求めなさい\n",
    "\n",
    "A.![image2](./calcuration_graph_image/IMG_2203.png)\n",
    "\n",
    "This is a forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Chain rule\n",
    "\n",
    "#### 5-2-1 Composite function\n",
    "$$\n",
    "z = t^2 \\\\\n",
    "t = x + y\n",
    "$$\n",
    "\n",
    "If That function was shown comosite function, it is represented by the product of the derivatives of each of the functions that make up the composite function\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma x} = \\frac{\\sigma z}{\\sigma t}  \\frac{\\sigma t}{\\sigma x} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\frac{\\sigma z}{\\sigma x}\n",
    "$$\n",
    "\n",
    "Calculate $\\frac{\\sigma z}{\\sigma x}$. However You must calculate partitial differential before\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma t} = 2t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma t}{\\sigma x} = 1\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{\\sigma z}{\\sigma x} = \\frac{\\sigma z}{\\sigma t} \\frac{\\sigma t}{\\sigma x} = 2t * 1 = 2(x + y)\n",
    "$$\n",
    "\n",
    "Calculate graph\n",
    "![image3](./calcuration_graph_image/IMG_2204.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2-2, How to calculate of  any node\n",
    "\n",
    "##### 1. Addition node\n",
    "- return same inputed value to next node\n",
    "\n",
    "##### 2. Multiplication node\n",
    "- Returns the value that is the flipped value of the input value at the time of forward propagation\n",
    "\n",
    "if you execute this one, you must save the input value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 Make a layer\n",
    "\n",
    "#### 5-3-1, Multilayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward(): Forward propagation\n",
    "# backward(): Backward propagation\n",
    "\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return int(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # flipped x , y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# debug\n",
    "print(int(price))\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-3-2 Addition layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return int(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n",
      "110 2.2 3.3 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(int(dapple_num), dapple, round(dorange, 1), dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6, Activation function layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1,ReLU layer\n",
    "\n",
    "- forward ver\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "x & {(x > 0)} \\\\\n",
    "0 & {(x \\geqq 0)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- backward ver\n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = \\begin{cases}\n",
    "1 & (x > 0) \\\\\n",
    "0 & (x \\geqq 0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "# mask: A list of Numpy as True or False\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5],[-2.0, 3.0]])\n",
    "# print(x)\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2 sigmoid layer\n",
    "\n",
    "- forward ver\n",
    "$$\n",
    "\\frac{1}{1 + exp(-x)}\n",
    "$$\n",
    "\n",
    "- backward ver\n",
    "\n",
    "### step1\n",
    "\n",
    "\"/\" node is shwon $ y = \\frac{1}{x}$ .\n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = -\\frac{1}{x^2} \\\n",
    " = -y^2\n",
    "$$\n",
    "\n",
    "### step2\n",
    "This is a \"+\" node. Therefore, returns the value obtained in step1 as it is\n",
    "\n",
    "### step3\n",
    "\n",
    "\"exp\" node is shown $ y = exp(x)$. So, in this gradient is \n",
    "$$\n",
    "\\frac{\\sigma y}{\\sigma x} = exp(x)\n",
    "$$\n",
    "\n",
    "### step4\n",
    "\"x\" node is invert and multiply the value at that time of forward propagation. \\\n",
    "Therefore, The result is following value\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma L}{\\sigma y} y^2 exp(-x)\n",
    "$$\n",
    "\n",
    "Thus, the formula obtained above can be organaized as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sigma L}{\\sigma y} y^2 exp(-x) &= \\frac{\\sigma L}{\\sigma y}\\frac{1}{(1 + exp(-x))^2}exp(-x) \\\\\n",
    "&= \\frac{\\sigma L}{\\sigma y}\\frac{1}{(1 + exp(-x))}\\frac{exp(-x)}{1 + exp(-x)} \\\\\n",
    "&= \\frac{\\sigma L}{\\sigma y} y(1-y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3 Affine layer\n",
    "\n",
    "In this sentence, Backpropagation of the matrix will be performed\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma L}{\\sigma X} = \\frac{\\sigma L}{\\sigma Y} * W^T \\\\\n",
    "\\frac{\\sigma L}{\\sigma W} = X^T * \\frac{\\sigma L}{\\sigma Y}\n",
    "$$\n",
    "\n",
    "calculation graph\n",
    "![image4](./calcuration_graph_image/IMG_2207.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3-1 Affine layer for batch version\n",
    "\n",
    "Basically the same but, be careful when adding bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "# forward example\n",
    "x_dot_w = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(x_dot_w)\n",
    "\n",
    "print(x_dot_w + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# backward example\n",
    "dy = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "print(dy)\n",
    "\n",
    "db = np.sum(dy, axis=0)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-4 Softmax-with-Loss layer\n",
    "\n",
    "It is usually using output layer\n",
    "\n",
    "example: Mnist data\n",
    "![image5](./calcuration_graph_image/IMG_2209.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Execute back propagation error method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from shared_code.layer import *\n",
    "from shared_code.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                weight_init_std=0.01):\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # Make a layer\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # x: input data  t:training data\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "\n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4b6cd190a39ea0ddbc5115907d95646639527cb5a2b4eb6fb2d78053b52a941"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
